{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Vgq3X7dm7pTw7YeEsD_tLLWQdV_as6Bm","authorship_tag":"ABX9TyPsswiRTm9r4cfrHaU9Cp/4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlSfac0rDMOc","executionInfo":{"status":"ok","timestamp":1682271657043,"user_tz":-540,"elapsed":14904,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"54abf290-8f7c-4359-e4a8-dc9cc262db5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","source":["**트랜포머 아키텍쳐**\n","1. 인코더\n","* 입력: 토큰의 시퀀스\n","* 출력: hidden State 또는 Context\n","\n","2. 디코더\n","* 입력: 인코더의 은닉 상태 + 직전까지 생성한 출력 토큰 서브 시퀀스\n","* 출력: 출력 토큰의 시퀀스를 한번에 하나씩 반복 생성\n","\n","*디코더의 구성*\n","* self-attention 레이어\n","* 인코더 디코더 어텐션 레이어: 현재 단계에서 이후에 생성될 토큰을 참조하지 않도록 한다 == 마스킹\n","* 피드포워드 레이어: 일반적인 전결합층/위 두개의 과정을 통해 얻은 정보를 바탕으로 출력의 형태를 결정한다\n","\n","\n","***트랜스포머 아키텍처 토큰 임베딩***\n","1. 입력 토큰을 one-hot 벡터로 변환한다. 이를 통해 각 토큰은 단 하나의 1값과 나머지는 모두 0 값으로 구성된 벡터로 표현한다.\n","2. one-hot 벡터를 선형 변환하여 연속 벡터 공간으로 매핑한다. **이를 통해 각 토큰은 연속적인 실수값으로 표현된다.**\n","3. 포지셔널 인코딩을 적용한다. 이를 통해 시퀀스 내에서 각 토큰의 상대적인 위치 정보를 인코딩할 수 있다. \n","\n","이러한 과정을 통해 입력 시퀀스의 각 토큰은 고차원의 연속 벡터로 표현된다. 이 과정에서 사용되는 선형 변환 매트릭스는 모델 학습 과정에서 업데이트되는 학습 가능한 파라미터이다.\n","\n","***트랜스포머 아키텍처 위치 임베딩***\n","\n","입력 시퀀스 내에서 각 토큰의 상대적인 위치 정보를 인코딩하는 기법이다. 이는 입력 시퀀스 내에서 각 토큰의 위치 정보를 반영하기 위해 사용된다.\n","\n","위치 임베딩은 포지셔널 인코딩 방식을 사용한다.\n","\n"],"metadata":{"id":"VU8yHiFJOvnN"}},{"cell_type":"markdown","source":["*인코더의 구성*\n","\n","인코더는 입력 시퀀스를 인코딩하는 부분으로, 다수의 층으로 구성되어있다. 각 층은 다수의 ***멀티 헤드 어텐션과 피드포워드 신경망 레이어***로 구성된다.\n","\n","인코더의 입력으로는 시퀀스의 각 *토큰에 해당하는 임베딩 벡터*와 *위치 임베딩 벡터*가 결합된 형태의 입력 시퀀스가 사용된다.\n","\n","이 입력 시퀀스는 각각의 인코더 층을 통과하면서 인코딩되어 최종적으로는 인코딩된 시퀀스가 출력된다.\n","\n","인코더의 각 층은 다음과 같은 두 가지 레이어로 구성된다.\n","\n","1. 멀티헤드 어텐션 레이어: 입력 시퀀스 내에서 각 토큰의 관계를 학습하기 위해 사용된다. 벡터를 이용해서 어텐션을 수행한다. *멀티 헤드 어텐션 레이어는 여러 개의 어텐션 헤드로 구성되어 있으며*, 이를 통해 다양한 과점에서 입력 시퀀스 내의 관계를 학습할 수 있다. \n","\n","2. 피드포워드 신경망 레이어: 멀티 헤드 어텐션 레이어를 통해 얻은 정보를 바탕으로 입력 시퀀스 내의 패턴을 학습하기 위해 사용된다. 이 레이어에서는 입력 벡터를 선형 변환하여 비선형 활성화 함수를 통해 처리한 뒤 다시 선형 변환하여 출력 벡터를 계산한다.\n","\n","\n","인코더에서 출력된 시퀀스는 디코더에서 입력으로 사용되며, 디코더는 이를 바탕으로 출력 시퀀스를 생성한다."],"metadata":{"id":"AdhoMDwBTw_2"}},{"cell_type":"markdown","source":["*인코더의 셀프 어텐션*\n","\n","인코더에서 사용되는 멀티 헤드 어텐션 레이어는 ***셀프 어텐션***을 기반으로 한다.\n","\n","셀프 어텐션은 입력 시퀀스 내의 각 토큰이 다른 토큰들과 어떤 관계를 가지는지를 학습하는 방법이다. \n","\n","입력 시퀀스의 각 토큰마다 **서로 다른 어텐션 헤드를 사용하여 다른 관점에서의 어텐션을 수행한다.** 이를 통해 입력 시퀀스 내의 다양한 학습을 할 수 있다.\n","\n","*스케일드 점곱 어텐션* -> 어텐션 메커니즘 중 하나\n","\n","스케일드 점곱 어텐션은 쿼리벡터와 키벡터의 내적 결과를 스케일링한 값에 소프트 맥스 함수를 적용하여 각각의 밸류벡터와 가중합을 구한다."],"metadata":{"id":"KV6WjDZ8Wf_r"}},{"cell_type":"markdown","source":["아래 코드는 스케일드 점곱 어텐션이다"],"metadata":{"id":"sMLI1TWDZ6UW"}},{"cell_type":"code","source":["#텍스트 토큰화\n","from transformers import AutoTokenizer\n","model_ckpt = \"bert-base-uncased\"\n","text = \"time flies like an arrow\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXe2OxAGIGsR","executionInfo":{"status":"ok","timestamp":1682271662526,"user_tz":-540,"elapsed":8,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"4221859f-eecd-49e4-e0f9-d19e6c2759be"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n","inputs.input_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfC01eauIPYk","executionInfo":{"status":"ok","timestamp":1682271672576,"user_tz":-540,"elapsed":9161,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"c09b97fc-da5c-46ce-bb98-6fe3bc6603fd"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2051, 10029,  2066,  2019,  8612]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["#밀집 임베딩 생성\n","from torch import nn\n","from transformers import AutoConfig\n","\n","config = AutoConfig.from_pretrained(model_ckpt)\n","token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n","token_emb\n","\n","#30522개의 정수 인덱스를 768 차원의 벡터로 매핑"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWqnyfXjIpr9","executionInfo":{"status":"ok","timestamp":1682271673686,"user_tz":-540,"elapsed":1115,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"4d92a5e0-8270-4fc1-ce56-a7e9ba9bc9d3"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(30522, 768)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["inputs_embeds = token_emb(inputs.input_ids)\n","inputs_embeds.size()\n","\n","#[batch_size, seq_len, hidden_dim]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eqllchAqIzPC","executionInfo":{"status":"ok","timestamp":1682271674025,"user_tz":-540,"elapsed":343,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"77c65060-98bd-4f19-9f1a-5d1da1263746"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import torch\n","from math import sqrt\n","\n","query = key = value = inputs_embeds #쿼리,키,값 벡터 제작\n","dim_k = key.size(-1) # 768\n","\n","#행렬 곱 계산\n","# [batch_size,m,n] X [batch_size,n,p] 모양일 때만 곱할 수 있다\n","\n","scores = torch.bmm(query,key.transpose(1,2)) / sqrt(dim_k)\n","scores.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujhXofgcam0E","executionInfo":{"status":"ok","timestamp":1682271674027,"user_tz":-540,"elapsed":34,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"19929854-c8ec-414a-dbd0-e6da5c7a8e14"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 5])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["#소프트 맥스 함수 적용\n","#각 행의 모든 열값을 더하면 1이 됨\n","import torch.nn.functional as F\n","\n","weights = F.softmax(scores, dim = -1)\n","weights.sum(dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLcwrXGWbHSH","executionInfo":{"status":"ok","timestamp":1682271674029,"user_tz":-540,"elapsed":29,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"e07f758e-b7cb-4caa-ab4a-0192751162a2"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["#값(value)에 가중치 적용\n","attn_outputs = torch.bmm(weights, value)\n","attn_outputs.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eEiHfxYxbldv","executionInfo":{"status":"ok","timestamp":1682271674030,"user_tz":-540,"elapsed":22,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"43a7ab34-ee2d-4b04-c549-0c0f251cab04"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["#하나의 함수로 표현\n","def scaled_dot_product_attention(query,key,value):\n","  dim_k = query.size(-1)\n","  scores = torch.bmm(query,key.transpose(1,2)) / sqrt(dim_k)\n","  weights = F.softmax(scores, dim=-1)\n","  \n","  return torch.bmm(weights,value)"],"metadata":{"id":"y50Zhg_pb3gp","executionInfo":{"status":"ok","timestamp":1682271674032,"user_tz":-540,"elapsed":17,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["**인코더: 멀티헤드 어텐션 구현**"],"metadata":{"id":"53uWpUc3dMXu"}},{"cell_type":"code","source":["class AttentionHead(nn.Module):\n","  def __init__(self, embed_dim, head_dim):\n","    #투영\n","    super().__init__()\n","    self.q = nn.Linaer(embed_dim,head_dim)\n","    self.k = nn.Linaer(embed_dim,head_dim)\n","    self.v = nn.Linaer(embed_dim,head_dim)\n","\n","  def forward(self, hidden_state):\n","    attn_outputs = scaled_dot_product_attention(self.q(hidden_state),self.k(hidden_state),self.v(hidden_state))\n","    return attn_outputs\n"],"metadata":{"id":"JJGuhrnPdTw5","executionInfo":{"status":"ok","timestamp":1682271681325,"user_tz":-540,"elapsed":269,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  #head_dim = 64 \n","  # embed_dim // num_heads == 768 /12 == 64\n","  def __init__(self, config):\n","    super.__init__()\n","    embed_dim = config.hidden_size\n","    num_heads = config.num_attention_heads\n","    head_dim = embed_dim // num_heads\n","    self.heads = nn.ModuleList([AttentionHead(embed_dim,head_dim) for _ in range(num_heads)])\n","    self.output_linaer = nn.Linear(embed_dim,embed_dim)\n","\n","  def forward(self, hidden_state):\n","    x = torch.cat([h(hidden_state) for h in self.heads], dim = -1)\n","    #마지막 차원 축으로 연결\n","    x = self.output_linear(x)\n","    return x"],"metadata":{"id":"yiuDrqM7eDro","executionInfo":{"status":"ok","timestamp":1682271681328,"user_tz":-540,"elapsed":9,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["#피드포워드 층\n","class FeedForward(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n","    self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n","    self.gelu == nn.GELU()\n","    self.dropout = nn.Dropout(config.hidden_droupout_prob)\n","\n","  def forward(self,x):\n","    x = self.linear_1(x)\n","    x = self.gelu(x)\n","    x = self.linear_2(x)\n","    x = self.dropout(x)\n","    return x\n","\n","#토큰의 위치에 상관없이 모든 토큰 임베딩에 병렬적으로 적용됨\n","\n","#GELU : 활성화 함수"],"metadata":{"id":"Xmnlth-zfOXR","executionInfo":{"status":"ok","timestamp":1682271682769,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["**인코더: 층 정규화**\n","\n","배치에 있는 각 입력 벡터를 평균이 0이고 분산이 1이 되도록 정규화\n","\n","* 사후 층 정규화\n","* 사전 층 정규화\n","\n","**스킵 연결**\n","\n","층을 통과시키지 않은 텐서를 층의 출력과 더하는 트리\n"," \n","error 의 축적 방지"],"metadata":{"id":"n6rwnWRQga3C"}},{"cell_type":"markdown","source":[],"metadata":{"id":"wDHhi79shPkl"}},{"cell_type":"code","source":["class TransformerEncoderLayer(nn.Module):\n","  def __init__(self,config):\n","    super().__init__()\n","    self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n","    self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n","    self.attention = MultiHeadAttention(config)\n","    self.feed_forward = FeedForward(config)\n","\n","  def forward(self,x):\n","    #층 정규화를 적용하고 입력을 쿼리,키,값으로 복사한다\n","    hidden_state = self.layer_norm_1(x)\n","    #어텐션에 스킵 연결을 적용한다\n","    x = x + self.attention(hidden_state)\n","    #스킵 연결과 피드 포워드 층을 적용한다.\n","    x = x + self.feed_forward(self.layer_norm_2(x))\n","    return x"],"metadata":{"id":"V_FIgafQgJ72","executionInfo":{"status":"ok","timestamp":1682271687259,"user_tz":-540,"elapsed":285,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["encoder_layer = TransformerEncoderLayer(config)\n","inputs_embeds.shape, encoder_layer(inputs_embeds).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":342},"id":"MDIyOkN6M4GV","executionInfo":{"status":"error","timestamp":1682271838479,"user_tz":-540,"elapsed":308,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"a4e35e29-d869-49e1-cf5a-4ebbee454b15"},"execution_count":19,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-85025a7a4450>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-3a4e387f5a3b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-54e4c90042b0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# embed_dim // num_heads == 768 /12 == 64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnum_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: descriptor '__init__' of 'super' object needs an argument"]}]},{"cell_type":"code","source":[],"metadata":{"id":"aHbKjsA8NZsz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**디코더**\n","\n","인코더와의 차이점\n","* 마스크드(masked)멀티 헤드 셀프 어텐션 층 -> 타임 스텝마다 지난 출력과 예측한 토큰만 사용하여 토큰 생성\n","\n","* 인코더 - 디코더 어텐션 층 -> 디코더의 중간 표현을 쿼리로 사용, 각 디코더 블록에서 인코더 스택의 출력 키와 값을 사용\n","\n"],"metadata":{"id":"8MxGX_hLOLm6"}},{"cell_type":"code","source":["#디코더: 마스크드 셀프 어텐션\n","seq_len = inputs.input_ids.size(-1)\n","mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n","mask[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMaOz_jcOn8E","executionInfo":{"status":"ok","timestamp":1682272131215,"user_tz":-540,"elapsed":313,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"28e62423-8094-45c5-9f18-03c5f2078c9d"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0., 0., 0.],\n","        [1., 1., 0., 0., 0.],\n","        [1., 1., 1., 0., 0.],\n","        [1., 1., 1., 1., 0.],\n","        [1., 1., 1., 1., 1.]])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["scores.masked_fill(mask == 0, -float(\"inf\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Os9Q7gd1O5SB","executionInfo":{"status":"ok","timestamp":1682272151963,"user_tz":-540,"elapsed":264,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"57aa4bec-9a8e-4d35-e672-66a78bdd9e79"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[27.1039,    -inf,    -inf,    -inf,    -inf],\n","         [-1.6533, 27.4362,    -inf,    -inf,    -inf],\n","         [-1.3001, -1.3387, 28.7270,    -inf,    -inf],\n","         [-0.9341,  0.6600, -2.3378, 27.8159,    -inf],\n","         [ 0.4928,  1.3075,  0.5093, -1.1198, 24.8107]]],\n","       grad_fn=<MaskedFillBackward0>)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["#스케일드 점곱 어텐션 함수 수정 : mask 반영\n","def scaled_dot_product_attention(query,key,value,mask=None):\n","  dim_k = query.size(-1)\n","  scores = torch.bmm(query,key.transpose(1,2)) / sqrt(dim_k)\n","  if mask is not None:\n","    scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n","  weights = F.softmax(scores, dim=-1)\n","  \n","  return torch.bmm(weights,value)"],"metadata":{"id":"A6KnBb1UO-YL","executionInfo":{"status":"ok","timestamp":1682272246309,"user_tz":-540,"elapsed":311,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UeqI60pDPVYb"},"execution_count":null,"outputs":[]}]}