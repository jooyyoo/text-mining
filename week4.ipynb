{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Vgq3X7dm7pTw7YeEsD_tLLWQdV_as6Bm","authorship_tag":"ABX9TyOKA6Enxz6JfzxdyRNhg3o7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlSfac0rDMOc","executionInfo":{"status":"ok","timestamp":1682236807946,"user_tz":-540,"elapsed":11211,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"85d4d0a8-5a24-4bb0-daa1-1784062ec65d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","source":["**트랜포머 아키텍쳐**\n","1. 인코더\n","* 입력: 토큰의 시퀀스\n","* 출력: hidden State 또는 Context\n","\n","2. 디코더\n","* 입력: 인코더의 은닉 상태 + 직전까지 생성한 출력 토큰 서브 시퀀스\n","* 출력: 출력 토큰의 시퀀스를 한번에 하나씩 반복 생성\n","\n","*디코더의 구성*\n","* self-attention 레이어\n","* 인코더 디코더 어텐션 레이어: 현재 단계에서 이후에 생성될 토큰을 참조하지 않도록 한다 == 마스킹\n","* 피드포워드 레이어: 일반적인 전결합층/위 두개의 과정을 통해 얻은 정보를 바탕으로 출력의 형태를 결정한다\n","\n","\n","***트랜스포머 아키텍처 토큰 임베딩***\n","1. 입력 토큰을 one-hot 벡터로 변환한다. 이를 통해 각 토큰은 단 하나의 1값과 나머지는 모두 0 값으로 구성된 벡터로 표현한다.\n","2. one-hot 벡터를 선형 변환하여 연속 벡터 공간으로 매핑한다. **이를 통해 각 토큰은 연속적인 실수값으로 표현된다.**\n","3. 포지셔널 인코딩을 적용한다. 이를 통해 시퀀스 내에서 각 토큰의 상대적인 위치 정보를 인코딩할 수 있다. \n","\n","이러한 과정을 통해 입력 시퀀스의 각 토큰은 고차원의 연속 벡터로 표현된다. 이 과정에서 사용되는 선형 변환 매트릭스는 모델 학습 과정에서 업데이트되는 학습 가능한 파라미터이다.\n","\n","***트랜스포머 아키텍처 위치 임베딩***\n","\n","입력 시퀀스 내에서 각 토큰의 상대적인 위치 정보를 인코딩하는 기법이다. 이는 입력 시퀀스 내에서 각 토큰의 위치 정보를 반영하기 위해 사용된다.\n","\n","위치 임베딩은 포지셔널 인코딩 방식을 사용한다.\n","\n"],"metadata":{"id":"VU8yHiFJOvnN"}},{"cell_type":"markdown","source":["*인코더의 구성*\n","\n","인코더는 입력 시퀀스를 인코딩하는 부분으로, 다수의 층으로 구성되어있다. 각 층은 다수의 ***멀티 헤드 어텐션과 피드포워드 신경망 레이어***로 구성된다.\n","\n","인코더의 입력으로는 시퀀스의 각 *토큰에 해당하는 임베딩 벡터*와 *위치 임베딩 벡터*가 결합된 형태의 입력 시퀀스가 사용된다.\n","\n","이 입력 시퀀스는 각각의 인코더 층을 통과하면서 인코딩되어 최종적으로는 인코딩된 시퀀스가 출력된다.\n","\n","인코더의 각 층은 다음과 같은 두 가지 레이어로 구성된다.\n","\n","1. 멀티헤드 어텐션 레이어: 입력 시퀀스 내에서 각 토큰의 관계를 학습하기 위해 사용된다. 벡터를 이용해서 어텐션을 수행한다. *멀티 헤드 어텐션 레이어는 여러 개의 어텐션 헤드로 구성되어 있으며*, 이를 통해 다양한 과점에서 입력 시퀀스 내의 관계를 학습할 수 있다. \n","\n","2. 피드포워드 신경망 레이어: 멀티 헤드 어텐션 레이어를 통해 얻은 정보를 바탕으로 입력 시퀀스 내의 패턴을 학습하기 위해 사용된다. 이 레이어에서는 입력 벡터를 선형 변환하여 비선형 활성화 함수를 통해 처리한 뒤 다시 선형 변환하여 출력 벡터를 계산한다.\n","\n","\n","인코더에서 출력된 시퀀스는 디코더에서 입력으로 사용되며, 디코더는 이를 바탕으로 출력 시퀀스를 생성한다."],"metadata":{"id":"AdhoMDwBTw_2"}},{"cell_type":"markdown","source":["*인코더의 셀프 어텐션*\n","\n","인코더에서 사용되는 멀티 헤드 어텐션 레이어는 ***셀프 어텐션***을 기반으로 한다.\n","\n","셀프 어텐션은 입력 시퀀스 내의 각 토큰이 다른 토큰들과 어떤 관계를 가지는지를 학습하는 방법이다. \n","\n","입력 시퀀스의 각 토큰마다 **서로 다른 어텐션 헤드를 사용하여 다른 관점에서의 어텐션을 수행한다.** 이를 통해 입력 시퀀스 내의 다양한 학습을 할 수 있다.\n","\n","*스케일드 점곱 어텐션* -> 어텐션 메커니즘 중 하나\n","\n","스케일드 점곱 어텐션은 쿼리벡터와 키벡터의 내적 결과를 스케일링한 값에 소프트 맥스 함수를 적용하여 각각의 밸류벡터와 가중합을 구한다."],"metadata":{"id":"KV6WjDZ8Wf_r"}},{"cell_type":"markdown","source":["아래 코드는 스케일드 점곱 어텐션이다"],"metadata":{"id":"sMLI1TWDZ6UW"}},{"cell_type":"code","source":["#텍스트 토큰화\n","from transformers import AutoTokenizer\n","model_ckpt = \"bert-base-uncased\"\n","text = \"time flies like an arrow\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXe2OxAGIGsR","executionInfo":{"status":"ok","timestamp":1682241529859,"user_tz":-540,"elapsed":312,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"d6748df1-5e22-4c23-fbf8-cf0950cb7ffa"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n","inputs.input_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfC01eauIPYk","executionInfo":{"status":"ok","timestamp":1682241505392,"user_tz":-540,"elapsed":338,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"5b1abdec-e995-45ed-fa99-5f053f883408"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2051, 10029,  2066,  2019,  8612]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["#밀집 임베딩 생성\n","from torch import nn\n","from transformers import AutoConfig\n","\n","config = AutoConfig.from_pretrained(model_ckpt)\n","token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n","token_emb\n","\n","#30522개의 정수 인덱스를 768 차원의 벡터로 매핑"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWqnyfXjIpr9","executionInfo":{"status":"ok","timestamp":1682241582186,"user_tz":-540,"elapsed":414,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"4f7980c3-7f26-4815-c5ca-20373c1a96c0"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(30522, 768)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["inputs_embeds = token_emb(inputs.input_ids)\n","inputs_embeds.size()\n","\n","#[batch_size, seq_len, hidden_dim]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eqllchAqIzPC","executionInfo":{"status":"ok","timestamp":1682241658277,"user_tz":-540,"elapsed":296,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"723eacbc-bb2a-4d20-e2f5-c076b3834b32"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import torch\n","from math import sqrt\n","\n","query = key = value = inputs_embeds #쿼리,키,값 벡터 제작\n","dim_k = key.size(-1) # 768\n","\n","#행렬 곱 계산\n","# [batch_size,m,n] X [batch_size,n,p] 모양일 때만 곱할 수 있다\n","\n","scores = torch.bmm(query,key.transpose(1,2)) / sqrt(dim_k)\n","scores.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujhXofgcam0E","executionInfo":{"status":"ok","timestamp":1682241779626,"user_tz":-540,"elapsed":323,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"a9724049-97a0-48bc-e249-d121d2ea189f"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 5])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["#소프트 맥스 함수 적용\n","#각 행의 모든 열값을 더하면 1이 됨\n","import torch.nn.functional as F\n","\n","weights = F.softmax(scores, dim = -1)\n","weights.sum(dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLcwrXGWbHSH","executionInfo":{"status":"ok","timestamp":1682241903222,"user_tz":-540,"elapsed":319,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"810b42db-a250-4509-a682-bf7da8f4dfc1"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["#값(value)에 가중치 적용\n","attn_outputs = torch.bmm(weights, value)\n","attn_outputs.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eEiHfxYxbldv","executionInfo":{"status":"ok","timestamp":1682241977361,"user_tz":-540,"elapsed":524,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"9213aa15-8105-4409-e705-fff61c0318e9"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["#하나의 함수로 표현\n","def scaled_dot_product_attention(query,key,value):\n","  dim_k = query.size(-1)\n","  scores = torch.bmm(query,key.transpose(1,2)) / sqrt(dim_k)\n","  weights = F.softmax(scores, dim=-1)\n","  \n","  return torch.bmm(weights,value)"],"metadata":{"id":"y50Zhg_pb3gp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**인코더: 멀티헤드 어텐션 구현**"],"metadata":{"id":"53uWpUc3dMXu"}},{"cell_type":"code","source":["class AttentionHead(nn.Module):\n","  def __init__(self, embed_dim, head_dim):\n","    #투영\n","    super().__init__()\n","    self.q = nn.Linaer(embed_dim,head_dim)\n","    self.k = nn.Linaer(embed_dim,head_dim)\n","    self.v = nn.Linaer(embed_dim,head_dim)\n","\n","  def forward(self, hidden_state):\n","    attn_outputs = scaled_dot_product_attention(self.q(hidden_state),self.k(hidden_state),self.v(hidden_state))\n","    return attn_outputs\n"],"metadata":{"id":"JJGuhrnPdTw5","executionInfo":{"status":"ok","timestamp":1682242548443,"user_tz":-540,"elapsed":305,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  #head_dim = 64 \n","  # embed_dim // num_heads == 768 /12 == 64\n","  def __init__(self, config):\n","    super.__init__()\n","    embed_dim = config.hidden_size\n","    num_heads = config.num_attention_heads\n","    head_dim = embed_dim // num_heads\n","    self.heads = nn.ModuleList([AttentionHead(embed_dim,head_dim) for _ in range(num_heads)])\n","    self.output_linaer = nn.Linear(embed_dim,embed_dim)\n","\n","  def forward(self, hidden_state):\n","    x = torch.cat([h(hidden_state) for h in self.heads], dim = -1)\n","    #마지막 차원 축으로 연결\n","    x = self.output_linear(x)\n","    return x"],"metadata":{"id":"yiuDrqM7eDro","executionInfo":{"status":"ok","timestamp":1682242853417,"user_tz":-540,"elapsed":292,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["#피드포워드 층\n","class FeedForward(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n","    self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n","    self.gelu == nn.GELU()\n","    self.dropout = nn.Dropout(config.hidden_droupout_prob)\n","\n","  def forward(self,x):\n","    x = self.linear_1(x)\n","    x = self.gelu(x)\n","    x = self.linear_2(x)\n","    x = self.dropout(x)\n","    return x\n","\n","#토큰의 위치에 상관없이 모든 토큰 임베딩에 병렬적으로 적용됨\n","\n","#GELU : 활성화 함수"],"metadata":{"id":"Xmnlth-zfOXR","executionInfo":{"status":"ok","timestamp":1682243101376,"user_tz":-540,"elapsed":11,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["**인코더: 층 정규화**\n","\n","배치에 있는 각 입력 벡터를 평균이 0이고 분산이 1이 되도록 정규화\n","\n","* 사후 층 정규화\n","* 사전 층 정규화\n","\n","**스킵 연결**\n","\n","층을 통과시키지 않은 텐서를 층의 출력과 더하는 트리\n"," \n","error 의 축적 방지"],"metadata":{"id":"n6rwnWRQga3C"}},{"cell_type":"markdown","source":[],"metadata":{"id":"wDHhi79shPkl"}},{"cell_type":"code","source":[],"metadata":{"id":"V_FIgafQgJ72"},"execution_count":null,"outputs":[]}]}