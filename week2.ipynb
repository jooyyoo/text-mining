{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPd6RPS9/yWuQ2fcviDBFeY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["트랜스포머\n","\n","**Transformer 아키텍쳐**\n","\n","새로운 아키텍쳐: 기계 번역의 품질과 훈련 비용 면에서 순환 신경망(RNN)을 능가\n","\n","**ULMFit**\n","\n","다양한 말뭉치로 부터 언어 모형 만든 후 적은양의 레이블 데이터에 대해 파인튜닝하여 최고 수준의 성능 이끌어 냄\n","\n","--> 이후 GPT BERT 등 발전의 촉매가 됨"],"metadata":{"id":"qY_fDs-tLG-b"}},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pQAS-GnNdZPD","executionInfo":{"status":"ok","timestamp":1682091559653,"user_tz":-540,"elapsed":18927,"user":{"displayName":"Jooyoung Yun","userId":"08346946677369879997"}},"outputId":"81d9516b-2a09-4882-fcb0-91dd1b780d99"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","source":["**GPT**\n","\n","Generative Pre Training\n","\n","**BERT**\n","\n","Pre-Training of Deep Bidirectional Transformers for Language Understanding\n","\n","\n","트랜스포머 아키텍처와 비지도 학습 결합하여 언어 모형 학습\n","\n","대부분의 NLP 벤치마크의 최고 성능 상회"],"metadata":{"id":"2QWfUhOjLvXE"}},{"cell_type":"markdown","source":["**인코더-디코더 프레임워크**\n","\n","트랜스포머 이전\n","\n"," NLP 의 SOTA\n","\n"," LSTM 등 순환 신경망 (RNN) 아키텍쳐\n","\n"," 1. 각 스텝에서 상태(state) 정보를 시퀀스의 다음 스텝으로 전달\n"," 2. 이전 스텝의 정보 추적하고 이를 사용해 예측"],"metadata":{"id":"IQjfvXLxMfVU"}},{"cell_type":"markdown","source":["**RNN**\n","\n","기계 번역 시스템 개발에서 중요한 역할을 수행함\n","\n","-인코딩: 인코더의 마지막 은닉 상태(last hidden state)\n","\n","-디코딩: 인코딩된 상태를 디코더의 입력으로 사용하여 출력 시퀀스 생성\n","\n","-RNN의 약점\n","1. 정보 병목: 인코딩 결과인 ***마지막 은닉 상태***가 전체 입력 시퀀스의 정보를 갈무리 해야함, 필연적 정보 손실\n","2. 순차적인 연산: 시퀀스 전체에 대한 병렬화에 취약"],"metadata":{"id":"iuZe6gt6d9iZ"}},{"cell_type":"markdown","source":["**어텐션 메커니즘**\n","1. 인코더의 각 스템의 은닉 상태를 디코더에서 모두 활용\n","2. 어텐션(attention): 인코더의 은닉 상태 출력에 가중치(어텐션)을 할당\n","\n","\n","\n","*   기계 번역에서의 어텐션\n","    \n","    원 문장과 번역 문장의 단어간 정렬 학습\n","\n","*   셀프 어텐션(self-attention):순환구조제거\n","    \n","    신경망의 같은 층의 모든 상태에 대해 어텐션 작동 (FF NN Feed Forword Neural Network)\n","\n","    병령성 확보: 자연어 처리 분야에서 큰 혁신을 불러옴\n","\n","\n"],"metadata":{"id":"pMq7pXyae26j"}},{"cell_type":"markdown","source":["**NLP 의 전이 학습**\n","\n","컴퓨터 비전에서의 전이 학습\n"," \n","ResNet 등 합성곱 신경망모델을 하나의 작업에 훈련 시킴\n","\n","이후 새로운 작업에 파인튜닝\n","\n","* 바디(body) 풍부한 원래 도메인에서 다양한 특성 학습\n","* 헤드(head) 새로운 도메인에 집중적으로 학습\n","\n","*지도 학습*\n","* 도메인마다 별도 학습\n","*전이 학습*\n","*정보가 풍부한 도메인에 학습된 모델\n","*새 도메인 학습 모델의 바디 초기화\n","*헤드는 적극 튜닝됨\n","*바디는 약하게 튜닝됨\n","\n","*장점*\n","* 주로 동일 양의 레이블 데이터의 지도 학습보다 정확도가 높음\n","\n","\n","\n","***NLP 대성공의 원동력***\n","* 어텐션 메커니즘\n","* RNN 의 순차 처리의 약점을 극복한 병렬연산\n","* 전이학습 정립\n","\n"],"metadata":{"id":"WEqkjzBKf3rU"}}]}