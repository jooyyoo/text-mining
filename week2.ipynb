{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMm/muCAoRZqIXtWtILD57Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["트랜스포머\n","\n","`<Transformer 아키텍쳐>`\n","\n","새로운 아키텍쳐: 기계 번역의 품질과 훈련 비용 면에서 순환 신경망(RNN)을 능가\n","\n","`ULMFit`\n","\n","다양한 말뭉치로 부터 언어 모형 만든 후 적은양의 레이블 데이터에 대해 파인튜닝하여 최고 수준의 성능 이끌어 냄\n","\n","--> 이후 GPT BERT 등 발전의 촉매가 됨"],"metadata":{"id":"qY_fDs-tLG-b"}},{"cell_type":"markdown","source":["**GPT**\n","\n","Generative Pre Training\n","\n","**BERT**\n","\n","Pre-Training of Deep Bidirectional Transformers for Language Understanding\n","\n","\n","트랜스포머 아키텍처와 비지도 학습 결합하여 언어 모형 학습\n","\n","대부분의 NLP 벤치마크의 최고 성능 상회"],"metadata":{"id":"2QWfUhOjLvXE"}},{"cell_type":"markdown","source":["**인코더-디코더 프레임워크**\n","\n","트랜스포머 이전\n","\n"," NLP 의 SOTA\n","\n"," LSTM 등 순환 신경망 (RNN) 아키텍쳐\n","\n"," 1. 각 스텝에서 상태(state) 정보를 시퀀스의 다음 스텝으로 전달\n"," 2. 이전 스텝의 정보 추적하고 이를 사용해 예측"],"metadata":{"id":"IQjfvXLxMfVU"}}]}